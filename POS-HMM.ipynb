{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNu4YOtFaiIQWpTXiYTUR0t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgVZUaEd1ivG","executionInfo":{"status":"ok","timestamp":1763836351849,"user_tz":-180,"elapsed":4405,"user":{"displayName":"Mouad Hamadou","userId":"05595019970978432021"}},"outputId":"fc317f0e-fa66-476a-bd06-34284546b590"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","#train sentences: 6075\n","#dev sentences:   909\n","#test sentences:  680\n","Building vocabulary...\n","Vocab size (freq>=2): 11886\n","Training HMM POS tagger...\n","Training done.\n","Evaluating on test set...\n","Test Accuracy: 0.8960\n","\n","Example predictions:\n","Sentence:  سوريا : تعديل وزاري واسع يشمل 8 حقائب\n","GOLD:      X PUNCT NOUN ADJ ADJ VERB NUM NOUN\n","PRED:      X PUNCT NOUN NOUN ADJ VERB NUM NOUN\n","----------------------------------------\n","Sentence:  دمشق ( وكالات الانباء ) - اجرى الرئيس السوري بشار الاسد تعديلا حكومياً واسعا تم ب موجب ه إقالة وزيري الداخلية و الإعلام عن منصبي ها في حين ظل محمد ناجي العطري رئيساً ل الحكومة .\n","GOLD:      X PUNCT NOUN NOUN PUNCT PUNCT VERB NOUN ADJ X X NOUN ADJ ADJ VERB ADP NOUN PRON NOUN NOUN ADJ CCONJ NOUN ADP NOUN PRON ADP ADP VERB X X X NOUN ADP NOUN PUNCT\n","PRED:      NOUN PUNCT NOUN NOUN PUNCT PUNCT VERB NOUN ADJ X X X PUNCT CCONJ VERB ADP NOUN PRON NOUN NOUN ADJ CCONJ NOUN ADP NOUN PRON ADP NOUN NOUN X X X X ADP NOUN PUNCT\n","----------------------------------------\n","Sentence:  و أضافت المصادر ان مهدي دخل الله رئيس تحرير صحيفة الحزب الحاكم و الليبرالي التوجهات تسلم منصب وزير الاعلام خلفا ل أحمد الحسن فيما تسلم اللواء غازي كنعان رئيس شعبة الامن السياسي منصب وزير الداخلية .\n","GOLD:      CCONJ VERB NOUN SCONJ X X X NOUN NOUN NOUN NOUN NOUN CCONJ ADJ NOUN VERB NOUN NOUN NOUN NOUN ADP NOUN X CCONJ VERB NOUN X X NOUN NOUN NOUN ADJ NOUN NOUN ADJ PUNCT\n","PRED:      CCONJ VERB NOUN SCONJ X X X NOUN NOUN NOUN NOUN ADJ CCONJ VERB NOUN NOUN NOUN NOUN NOUN ADJ ADP X PUNCT CCONJ VERB NOUN X X NOUN NOUN NOUN ADJ NOUN NOUN ADJ PUNCT\n","----------------------------------------\n","Sentence:  و ذكرت وكالة الانباء السورية ان التعديل شمل ثماني حقائب بين ها وزارتا الداخلية و الاقتصاد .\n","GOLD:      CCONJ VERB NOUN NOUN ADJ SCONJ NOUN VERB NUM NOUN ADP PRON NOUN ADJ CCONJ NOUN PUNCT\n","PRED:      CCONJ VERB NOUN NOUN ADJ SCONJ NOUN NOUN ADP NOUN ADP PRON NOUN ADJ CCONJ NOUN PUNCT\n","----------------------------------------\n","Sentence:  و عين اللواء كنعان الذي كان رئيسا ل جهاز الامن السياسي وزيرا ل الداخلية خلفا ل لواء علي حمود .\n","GOLD:      CCONJ VERB X X X AUX NOUN ADP NOUN NOUN ADJ NOUN ADP ADJ NOUN ADP NOUN X X PUNCT\n","PRED:      CCONJ VERB NOUN ADJ DET VERB NOUN ADP NOUN NOUN ADJ ADJ ADP ADJ ADJ ADP NOUN ADP X PUNCT\n","----------------------------------------\n"]}],"source":["import math\n","from collections import defaultdict, Counter\n","\n","# =========================\n","# CONFIG\n","# =========================\n","\n","# Change these paths to where you saved the .conll files\n","TRAIN_PATH = \"ar_padt-ud-train.conll\"\n","DEV_PATH   = \"ar_padt-ud-dev.conll\"   # not strictly needed for basic version\n","TEST_PATH  = \"ar_padt-ud-test.conll\"\n","\n","UNK_TOKEN = \"<UNK>\"\n","SMOOTHING = 1.0  # Laplace smoothing\n","\n","\n","# =========================\n","# DATA LOADING\n","# =========================\n","\n","def read_conll(path):\n","    \"\"\"\n","    Reads a simple CoNLL file: word[TAB]tag, sentences separated by blank lines.\n","    Returns a list of sentences, each sentence is a list of (word, tag) pairs.\n","    \"\"\"\n","    sentences = []\n","    current = []\n","\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            line = line.strip()\n","            if not line:\n","                if current:\n","                    sentences.append(current)\n","                    current = []\n","                continue\n","            parts = line.split(\"\\t\")\n","            if len(parts) != 2:\n","                # skip malformed lines\n","                continue\n","            word, tag = parts\n","            current.append((word, tag))\n","\n","    if current:\n","        sentences.append(current)\n","\n","    return sentences\n","\n","\n","def build_vocab(train_sentences, min_freq=2):\n","    \"\"\"\n","    Build vocabulary from training sentences.\n","    Words with frequency < min_freq become UNK.\n","    Returns:\n","      - vocab: set of known words (excluding UNK)\n","      - word2freq: full frequency dictionary\n","    \"\"\"\n","    freq = Counter()\n","    for sent in train_sentences:\n","        for word, _ in sent:\n","            freq[word] += 1\n","\n","    vocab = {w for w, c in freq.items() if c >= min_freq}\n","    return vocab, freq\n","\n","\n","def replace_rare_with_unk(sentences, vocab):\n","    \"\"\"\n","    Replace words not in vocab with UNK_TOKEN.\n","    Returns new list of sentences.\n","    \"\"\"\n","    new_sents = []\n","    for sent in sentences:\n","        new_sent = []\n","        for word, tag in sent:\n","            if word not in vocab:\n","                new_sent.append((UNK_TOKEN, tag))\n","            else:\n","                new_sent.append((word, tag))\n","        new_sents.append(new_sent)\n","    return new_sents\n","\n","\n","# =========================\n","# HMM TRAINING\n","# =========================\n","\n","class HMMPOSTagger:\n","    def __init__(self, smoothing=1.0):\n","        self.smoothing = smoothing\n","        self.tags = []\n","        self.tag_to_idx = {}\n","        self.idx_to_tag = {}\n","        self.vocab = set()\n","\n","        # counts\n","        self.tag_counts = Counter()\n","        self.initial_tag_counts = Counter()\n","        self.transition_counts = defaultdict(Counter)  # prev_tag -> Counter(next_tag)\n","        self.emission_counts = defaultdict(Counter)    # tag -> Counter(word)\n","\n","        # probabilities (log)\n","        self.log_initial = {}\n","        self.log_transition = defaultdict(dict)\n","        self.log_emission = defaultdict(dict)\n","\n","    def fit(self, sentences):\n","        \"\"\"\n","        sentences: list of sentences, each sentence is list of (word, tag).\n","        \"\"\"\n","\n","        # Count tags, transitions, emissions\n","        for sent in sentences:\n","            if not sent:\n","                continue\n","\n","            # initial tag\n","            first_tag = sent[0][1]\n","            self.initial_tag_counts[first_tag] += 1\n","\n","            prev_tag = None\n","            for word, tag in sent:\n","                self.tag_counts[tag] += 1\n","                self.emission_counts[tag][word] += 1\n","                self.vocab.add(word)\n","\n","                if prev_tag is not None:\n","                    self.transition_counts[prev_tag][tag] += 1\n","                prev_tag = tag\n","\n","        # list of unique tags\n","        self.tags = sorted(self.tag_counts.keys())\n","        self.tag_to_idx = {t: i for i, t in enumerate(self.tags)}\n","        self.idx_to_tag = {i: t for t, i in self.tag_to_idx.items()}\n","\n","        num_tags = len(self.tags)\n","        vocab_size = len(self.vocab) + 1  # +1 for UNK if needed\n","\n","        # Initial probabilities P(tag at position 0)\n","        total_initial = sum(self.initial_tag_counts.values())\n","        for tag in self.tags:\n","            count = self.initial_tag_counts[tag]\n","            # Laplace smoothing\n","            prob = (count + self.smoothing) / (total_initial + self.smoothing * num_tags)\n","            self.log_initial[tag] = math.log(prob)\n","\n","        # Transition probabilities P(tag_i | tag_{i-1})\n","        for prev_tag in self.tags:\n","            total_prev = sum(self.transition_counts[prev_tag].values())\n","            for next_tag in self.tags:\n","                count = self.transition_counts[prev_tag][next_tag]\n","                prob = (count + self.smoothing) / (total_prev + self.smoothing * num_tags)\n","                self.log_transition[prev_tag][next_tag] = math.log(prob)\n","\n","        # Emission probabilities P(word | tag)\n","        for tag in self.tags:\n","            total_tag = sum(self.emission_counts[tag].values())\n","            for word in self.vocab:\n","                count = self.emission_counts[tag][word]\n","                prob = (count + self.smoothing) / (total_tag + self.smoothing * vocab_size)\n","                self.log_emission[tag][word] = math.log(prob)\n","\n","            # probability for UNK word for this tag\n","            unk_count = 0\n","            prob_unk = (unk_count + self.smoothing) / (total_tag + self.smoothing * vocab_size)\n","            self.log_emission[tag][UNK_TOKEN] = math.log(prob_unk)\n","\n","    def viterbi(self, words):\n","        \"\"\"\n","        Viterbi decoding to find best tag sequence for a list of words.\n","        words: list of tokens (strings).\n","        Returns: list of tags (strings).\n","        \"\"\"\n","        T = len(words)\n","        N = len(self.tags)\n","\n","        # Convert OOV words to UNK\n","        obs = [w if w in self.vocab else UNK_TOKEN for w in words]\n","\n","        # dp[t][i] = max log prob of a path ending in tag i at position t\n","        dp = [[-math.inf] * N for _ in range(T)]\n","        backpointer = [[None] * N for _ in range(T)]\n","\n","        # Initialization (t = 0)\n","        for i, tag in enumerate(self.tags):\n","            log_init = self.log_initial.get(tag, -math.inf)\n","            log_emit = self.log_emission[tag].get(obs[0], -math.inf)\n","            dp[0][i] = log_init + log_emit\n","            backpointer[0][i] = None\n","\n","        # Recursion\n","        for t in range(1, T):\n","            for i, curr_tag in enumerate(self.tags):\n","                best_score = -math.inf\n","                best_prev = None\n","                log_emit = self.log_emission[curr_tag].get(obs[t], -math.inf)\n","\n","                for j, prev_tag in enumerate(self.tags):\n","                    log_trans = self.log_transition[prev_tag].get(curr_tag, -math.inf)\n","                    score = dp[t-1][j] + log_trans + log_emit\n","                    if score > best_score:\n","                        best_score = score\n","                        best_prev = j\n","\n","                dp[t][i] = best_score\n","                backpointer[t][i] = best_prev\n","\n","        # Termination\n","        best_last_score = -math.inf\n","        best_last_idx = None\n","        for i in range(N):\n","            if dp[T-1][i] > best_last_score:\n","                best_last_score = dp[T-1][i]\n","                best_last_idx = i\n","\n","        # Backtrack\n","        best_path_idx = [best_last_idx]\n","        for t in range(T-1, 0, -1):\n","            best_prev = backpointer[t][best_path_idx[-1]]\n","            best_path_idx.append(best_prev)\n","        best_path_idx.reverse()\n","\n","        best_tags = [self.idx_to_tag[i] for i in best_path_idx]\n","        return best_tags\n","\n","    def predict_sentence(self, sentence):\n","        \"\"\"\n","        sentence: list of words\n","        returns: list of predicted tags\n","        \"\"\"\n","        return self.viterbi(sentence)\n","\n","    def evaluate(self, sentences):\n","        \"\"\"\n","        sentences: list of sentences, each is list of (word, gold_tag)\n","        returns: accuracy\n","        \"\"\"\n","        correct = 0\n","        total = 0\n","\n","        for sent in sentences:\n","            words = [w for w, _ in sent]\n","            gold_tags = [t for _, t in sent]\n","            pred_tags = self.predict_sentence(words)\n","\n","            for g, p in zip(gold_tags, pred_tags):\n","                if g == p:\n","                    correct += 1\n","                total += 1\n","\n","        return correct / total if total > 0 else 0.0\n","\n","\n","# =========================\n","# MAIN\n","# =========================\n","\n","def main():\n","    print(\"Loading data...\")\n","    train_sents = read_conll(TRAIN_PATH)\n","    dev_sents = read_conll(DEV_PATH)\n","    test_sents = read_conll(TEST_PATH)\n","\n","    print(f\"#train sentences: {len(train_sents)}\")\n","    print(f\"#dev sentences:   {len(dev_sents)}\")\n","    print(f\"#test sentences:  {len(test_sents)}\")\n","\n","    # Build vocabulary from training data\n","    print(\"Building vocabulary...\")\n","    vocab, freq = build_vocab(train_sents, min_freq=2)\n","    print(f\"Vocab size (freq>=2): {len(vocab)}\")\n","\n","    # Replace rare / unknown words with UNK in train/dev/test\n","    train_sents_unk = replace_rare_with_unk(train_sents, vocab)\n","    dev_sents_unk   = replace_rare_with_unk(dev_sents, vocab)\n","    test_sents_unk  = replace_rare_with_unk(test_sents, vocab)\n","\n","    print(\"Training HMM POS tagger...\")\n","    tagger = HMMPOSTagger(smoothing=SMOOTHING)\n","    tagger.fit(train_sents_unk)\n","    print(\"Training done.\")\n","\n","    print(\"Evaluating on test set...\")\n","    acc = tagger.evaluate(test_sents_unk)\n","    print(f\"Test Accuracy: {acc:.4f}\")\n","\n","    # Show some example predictions\n","    print(\"\\nExample predictions:\")\n","    for sent in test_sents[:5]:\n","        words = [w for w, _ in sent]\n","        gold  = [t for _, t in sent]\n","        pred  = tagger.predict_sentence(words)\n","        print(\"Sentence: \", \" \".join(words))\n","        print(\"GOLD:     \", \" \".join(gold))\n","        print(\"PRED:     \", \" \".join(pred))\n","        print(\"-\" * 40)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}